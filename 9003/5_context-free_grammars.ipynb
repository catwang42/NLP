{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Free Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NTLK has a module for building context-free grammars from a string representation of the rules. Let's start by creating our toy grammar from the lecture, and generating the four possible sentences that can be derived using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.parse.generate import generate\n",
    "toy_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> VBG NP \n",
    "  VBG -> \"ate\"\n",
    "  NP -> DT NN\n",
    "  DT -> \"the\"\n",
    "  NN -> \"rat\" | \"cheese\" \n",
    "  \"\"\")\n",
    "\n",
    "for sentence in generate(toy_grammar):\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's examine our grammar a bit. We can iterate through the rules (productions), look at the symbols on their left hand and right hand sides, and see whether they're terminals or non-terminals. We'll do this to create a list of terminals and non-terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "terminals = set()\n",
    "nonterminals = set()\n",
    "for production in toy_grammar.productions():\n",
    "    print production\n",
    "    print production.lhs()\n",
    "    print production.rhs()\n",
    "    for symbol in production.rhs():\n",
    "        if nltk.grammar.is_terminal(symbol):\n",
    "            terminals.add(symbol)\n",
    "        else:\n",
    "            nonterminals.add(symbol)\n",
    "\n",
    "print \"terminals\"\n",
    "print terminals\n",
    "print \"nonterminals\"\n",
    "print nonterminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use Early parsing to parse a sentence using our grammar. NLTK has a built-in version of the algorithm that you can use. We can set trace=2 to show the steps that the model is taking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = nltk.parse.EarleyChartParser(toy_grammar, trace=2)\n",
    "for parse in parser.parse(\"the rat ate the cheese\".split()):\n",
    "    print parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are exactly the same steps as we saw in lecture, except this version has the prediction of lexical rules such as NN -> 'cheese' in the step before scanning them. You should look through these steps again to make sure you understand them.\n",
    "\n",
    "Let's try to go beyond our toy grammar. Building one by hand would be painful, but fortunately NLTK contains a portion of the human-annotated Penn Treebank, which we can use for grammar induction. First, though, let's take a look at some trees from the Penn Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tree in nltk.corpus.treebank.parsed_sents()[0:5]:\n",
    "      print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with the default Penn Treebank annotation is that some of its nonterminals include sentential grammatical role labels which don't make sense for building a CFG grammar (for instance, NP-SBJ for NPs as the subject as the sentence). Let's write a function to remove them. To do this, we'll have to traverse these trees, and change the labels on the nodes. This is fairly easy, since iterating over a NLTK tree with a for loop means to interate over its children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_grammatical_roles(tree):\n",
    "    try:\n",
    "        if \"-\" in tree.label():\n",
    "            tree.set_label(tree.label().split(\"-\")[0])\n",
    "    except: #we've hit a terminal node\n",
    "        return \n",
    "    for child in tree:\n",
    "        remove_grammatical_roles(child)\n",
    "\n",
    "for tree in nltk.corpus.treebank.parsed_sents()[0:5]:\n",
    "    remove_grammatical_roles(tree)\n",
    "    print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's build a new grammar. NLTK trees have a handy method (productions()) which gives you all the CFG rules for the tree. We can collect these rules across all the texts to build a new CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.grammar import CFG,Nonterminal\n",
    "\n",
    "productions = set()\n",
    "\n",
    "for tree in nltk.corpus.treebank.parsed_sents():\n",
    "    remove_grammatical_roles(tree)\n",
    "    for production in tree.productions():\n",
    "        productions.add(production)\n",
    "treebank_grammar = CFG(Nonterminal('S'), list(productions))\n",
    "print len(treebank_grammar.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas our old grammar had 7 rules, this one has over 17 thousand. Many of them, however, are the lexical rules which produce terminals nodes. Let's see how many of them are non-lexical by counting only those whose RHS is not a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonlex_count = 0\n",
    "for production in treebank_grammar.productions():\n",
    "    if not nltk.grammar.is_terminal(production.rhs()[0]):\n",
    "        nonlex_count +=1\n",
    "print nonlex_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's still a lot of rules. Let's see if it can parse our original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = nltk.parse.EarleyChartParser(treebank_grammar, trace=0)\n",
    "for parse in parser.parse(\"the rat ate the cheese\".split()):\n",
    "    print parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, because it was built on Wall Street Journal texts, it doesn't know about rats, cheese, or even eating, these words are out of vocabulary (OOV). Unfortunately, even when the vocabulary of sentence is covered, the grammar is completely unusable (in this context). To demonstrate why, try running the below, though note that it might crash your iPython notebook session. We have switched to a bottom up parser which works a bit better in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = nltk.parse.BottomUpChartParser(treebank_grammar, trace=0)\n",
    "for parse in parser.parse(\"revenue increased last quarter\".split()):\n",
    "    print parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, way, way, way too many possible parses, almost all of which are absolute junk. Toy grammars make parsing look easy, but parsing with a large grammar involves huge amounts of ambiguity: you need ways to filter out the junk to find the correct parse. One simple thing you might try if you'd like to play around some more is filtering your grammar based on frequency, so you're only using the core rules of the grammar. Can you bring down the range of possibilities to a reasonable set without eliminating the correct parse?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
