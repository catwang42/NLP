{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use NLTK to access WordNet, look at some senses and lexical relations, and find paths between words. First, let's load NLTK and make sure WordNet is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print wn.readme(lang=\"eng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in lecture, the main nodes in WordNet are synsets, not words. Given any word, we can access relevant synsets using the synsets commands. We can optionally limit to a particular word category (n = noun, v = verb, a = adjective, r = adverb). For each of the synsets of the word type \"class\", let's look at their definition, their corresponding lemmas, an example of their usage, and their hypernyms (often only one, but can be multiple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets(\"book\",\"n\"):\n",
    "    print synset.name()\n",
    "    print synset.definition()\n",
    "    print synset.lemma_names()\n",
    "    print synset.examples()\n",
    "    print synset.hypernyms()\n",
    "    print \"-------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here why WordNet is sometimes seen as too fine-grained, particularly for word sense disambiguation; several of these senses are closely related to each other in meaning. WordNet does not ditinguish between true homonyms, and instances of polysemy. In any case, once we know its name, we can access a particular synset with the synset command, and look at other relationships, such as hyponyms; Note that meronyms and holonyms come in three types: part, member or substance, though we'll only look at part here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wn.synset(\"book.n.02\").hyponyms()\n",
    "print wn.synset(\"book.n.02\").part_meronyms()\n",
    "print wn.synset(\"book.n.10\").part_holonyms() # \"book\" meaning a division of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synset has a set of lemmas associated with it. Since antonyms are often specific to the word form, they are defined on lemmas, not synsets. Another function, derivationally_related_forms gives other lemmas which are related by derivational morphology, though this is not comprehensive. Finally, lemmas have a count associated with them, derived from a sense tagged corpus: these can be used to identify which senses of a word are more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wn.synsets(\"happy\")[0]\n",
    "print wn.synsets(\"happy\")[0].lemmas()[0].antonyms()\n",
    "print wn.synsets(\"happy\")[0].lemmas()[0].derivationally_related_forms()\n",
    "print wn.synsets(\"happy\")[0].lemmas()[0].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic similarity measures mentioned in class (and several others) are available in the NLTK WordNet interface, as are other functions which are used in their derivation. For similarity metrics which require information content, we can load statistics from available corpora (the SEMCOR and Brown corpora are popular options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "print wn.synset(\"book.n.02\").path_similarity(wn.synset(\"newspaper.n.03\"))\n",
    "print wn.synset(\"book.n.02\").wup_similarity(wn.synset(\"newspaper.n.03\"))\n",
    "\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "print wn.synset(\"book.n.02\").lin_similarity(wn.synset(\"newspaper.n.03\"),semcor_ic) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, they are somewhat opaque in their operation, and only work on synsets. Let's create a version of basic path distance which doesn't require you to select a specific synset in advance, and shows you the exact path through the WordNet heirarchy that the score is based on. There are many ways to do this, we'll do it in a fairly clear but not entirely optimal way. First, given a set of synsets, let's get a dictionary where the keys correspond to all hypernym synsets, and the values are the next step below on the shortest past to one of the initial synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hypernym_path_dict(synsets):\n",
    "    hypernym_dict = {}\n",
    "    synsets_to_expand = synsets\n",
    "    while synsets_to_expand:\n",
    "        new_synsets_to_expand = set()\n",
    "        for synset in synsets_to_expand:\n",
    "            for hypernym in synset.hypernyms():\n",
    "                if hypernym not in hypernym_dict:  # this ensures we get the shortest path\n",
    "                    hypernym_dict[hypernym] = synset\n",
    "                    new_synsets_to_expand.add(hypernym)\n",
    "        synsets_to_expand = new_synsets_to_expand\n",
    "    return hypernym_dict\n",
    "        \n",
    "        \n",
    "hypernym_dict = get_hypernym_path_dict(wn.synsets(\"book\",\"n\"))\n",
    "print hypernym_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to build the path using this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_path_using_hypernym_dict(hypernym,hypernym_dict,synsets):\n",
    "    path = [hypernym]\n",
    "    current_synset = hypernym_dict[hypernym]\n",
    "    while current_synset not in synsets:\n",
    "        path.append(current_synset)\n",
    "        current_synset =  hypernym_dict[current_synset]\n",
    "    path.append(current_synset)\n",
    "    return path\n",
    "    \n",
    "print get_path_using_hypernym_dict(wn.synset('physical_entity.n.01'),hypernym_dict,wn.synsets(\"book\",\"n\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can build ancestor dictionaries for each of the words, look at the intersection, and then find the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_shortest_path_between(word1,word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    hypernym_dict1 = get_hypernym_path_dict(synsets1)\n",
    "    hypernym_dict2 = get_hypernym_path_dict(synsets2)\n",
    "    best_path = []\n",
    "    for hypernym in hypernym_dict1:\n",
    "        if hypernym in hypernym_dict2 and hypernym_dict1[hypernym] != hypernym_dict2[hypernym]:\n",
    "            path1 = get_path_using_hypernym_dict(hypernym,hypernym_dict1,synsets1)\n",
    "            path2 = get_path_using_hypernym_dict(hypernym,hypernym_dict2,synsets2)\n",
    "            if not best_path or len(path1) + len(path2) - 1 < len(best_path):\n",
    "                path1.reverse()\n",
    "                best_path = path1 + path2[1:]\n",
    "    return best_path\n",
    "\n",
    "path = get_shortest_path_between(\"book\",\"newspaper\")\n",
    "print 1.0/len(path)\n",
    "print path    \n",
    "\n",
    "path = get_shortest_path_between(\"dog\",\"cat\")\n",
    "print 1.0/len(path)\n",
    "print path  \n",
    "\n",
    "path = get_shortest_path_between(\"nickel\",\"money\")\n",
    "print 1.0/len(path)\n",
    "print path  \n",
    "\n",
    "path = get_shortest_path_between(\"computer\",\"pizza\")\n",
    "print 1.0/len(path)\n",
    "print path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest path does not always correspond to the most obvious relationship between two words: for instance, newspaper and book are join as products (not reading materials), dog and cat by informal senses related to people, rather than animals. Using depth and information-content basic metrics can improve this situation. Another approach is to use the counts of lemmas to ignore rare senses. Note that doing all this for other metrics is somewhat different, because they are based on the idea of lowest common subsumer, which is not necessarily on the shortest path."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
